{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "def load_and_preprocess_data(file_path):\n",
    "    \"\"\"\n",
    "    Load and preprocess the PM2.5 dataset\n",
    "    \"\"\"\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Handle missing values\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def engineer_features(df):\n",
    "    \"\"\"\n",
    "    Create engineered features for air quality prediction\n",
    "    \"\"\"\n",
    "    # 1. Temporal Features\n",
    "    df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "    df['day_of_week'] = df['date'].dt.dayofweek\n",
    "    df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "    \n",
    "    # 2. Wind-related Features\n",
    "    # Create a numeric representation of wind direction\n",
    "    wind_dir_map = {\n",
    "        'NW': 1, 'NE': 2, 'SW': 3, 'SE': 4\n",
    "    }\n",
    "    df['wind_dir_numeric'] = df['cbwd'].map(wind_dir_map)\n",
    "    \n",
    "    # 3. Interaction Features\n",
    "    # Temperature and Humidity Interaction\n",
    "    df['temp_humidity_index'] = df['TEMP'] * df['HUMI'] / 100\n",
    "    \n",
    "    # Precipitation and Pressure Interaction\n",
    "    df['precip_pressure_index'] = df['precipitation'] / (df['PRES'] / 1000)\n",
    "    \n",
    "    # 4. Rolling Window Features\n",
    "    # Short-term and medium-term trends\n",
    "    df['temp_3hr_rolling_avg'] = df.groupby('season')['TEMP'].rolling(window=3, min_periods=1).mean().reset_index(0, drop=True)\n",
    "    df['pm25_24hr_rolling_avg'] = df.groupby('season')['PM'].rolling(window=24, min_periods=1).mean().reset_index(0, drop=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def prepare_data(df):\n",
    "    \"\"\"\n",
    "    Prepare data for modeling with appropriate feature selection\n",
    "    \"\"\"\n",
    "    # Engineer features\n",
    "    df = engineer_features(df)\n",
    "    \n",
    "    # Select features\n",
    "    numeric_features = [\n",
    "        'TEMP', 'DEWP', 'HUMI', 'PRES', \n",
    "        'Iws', 'precipitation', 'Iprec',\n",
    "        'wind_dir_numeric', \n",
    "        'temp_humidity_index', \n",
    "        'precip_pressure_index',\n",
    "        'temp_3hr_rolling_avg',\n",
    "        'pm25_24hr_rolling_avg'\n",
    "    ]\n",
    "    \n",
    "    categorical_features = ['season', 'cbwd']\n",
    "    \n",
    "    # Prepare features and target\n",
    "    X = df[numeric_features + categorical_features]\n",
    "    y = df['PM']\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def create_preprocessing_pipeline(X):\n",
    "    \"\"\"\n",
    "    Create a preprocessing pipeline with appropriate transformers\n",
    "    \"\"\"\n",
    "    # Identify numeric and categorical columns\n",
    "    numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_features = X.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    # Create preprocessor\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numeric_features),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "        ])\n",
    "    \n",
    "    return preprocessor\n",
    "\n",
    "def train_models(X, y):\n",
    "    \"\"\"\n",
    "    Train and evaluate multiple models with preprocessing\n",
    "    \"\"\"\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Create preprocessor\n",
    "    preprocessor = create_preprocessing_pipeline(X_train)\n",
    "    \n",
    "    # Models and their hyperparameter grids\n",
    "    models = {\n",
    "        'Random Forest': {\n",
    "            'model': RandomForestRegressor(random_state=42),\n",
    "            'params': {\n",
    "                'regressor__n_estimators': [50, 100, 200],\n",
    "                'regressor__max_depth': [None, 10, 20],\n",
    "                'regressor__min_samples_split': [2, 5, 10]\n",
    "            }\n",
    "        },\n",
    "        'Gradient Boosting': {\n",
    "            'model': GradientBoostingRegressor(random_state=42),\n",
    "            'params': {\n",
    "                'regressor__n_estimators': [50, 100, 200],\n",
    "                'regressor__learning_rate': [0.01, 0.1, 0.2],\n",
    "                'regressor__max_depth': [3, 4, 5]\n",
    "            }\n",
    "        },\n",
    "        'Support Vector Regression': {\n",
    "            'model': SVR(),\n",
    "            'params': {\n",
    "                'regressor__kernel': ['rbf', 'linear'],\n",
    "                'regressor__C': [0.1, 1, 10],\n",
    "                'regressor__epsilon': [0.1, 0.2, 0.3]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Results storage\n",
    "    results = {}\n",
    "    \n",
    "    # Train and evaluate each model\n",
    "    for name, config in models.items():\n",
    "        # Create pipeline\n",
    "        pipeline = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('regressor', config['model'])\n",
    "        ])\n",
    "        \n",
    "        # Grid Search with Cross-Validation\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=pipeline, \n",
    "            param_grid={**config['params']},\n",
    "            cv=5, \n",
    "            scoring='neg_mean_absolute_error'\n",
    "        )\n",
    "        \n",
    "        # Fit the grid search\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        # Best model predictions\n",
    "        y_pred = grid_search.predict(X_test)\n",
    "        \n",
    "        # Evaluation metrics\n",
    "        results[name] = {\n",
    "            'best_params': grid_search.best_params_,\n",
    "            'mae': mean_absolute_error(y_test, y_pred),\n",
    "            'rmse': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "            'r2': r2_score(y_test, y_pred)\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def visualize_results(df):\n",
    "    \"\"\"\n",
    "    Create visualizations to understand data and features\n",
    "    \"\"\"\n",
    "    # Correlation heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    correlation_matrix = df.select_dtypes(include=['float64', 'int64']).corr()\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "    plt.title('Feature Correlation Heatmap')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # PM2.5 distribution by season\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(x='season', y='PM', data=df)\n",
    "    plt.title('PM2.5 Distribution by Season')\n",
    "    plt.show()\n",
    "\n",
    "def main(file_path):\n",
    "    # 1. Load the data\n",
    "    df = load_and_preprocess_data(file_path)\n",
    "    \n",
    "    # 2. Visualize initial insights\n",
    "    visualize_results(df)\n",
    "    \n",
    "    # 3. Prepare data for modeling\n",
    "    X, y = prepare_data(df)\n",
    "    \n",
    "    # 4. Train and evaluate models\n",
    "    model_results = train_models(X, y)\n",
    "    \n",
    "    # 5. Print results\n",
    "    for model_name, results in model_results.items():\n",
    "        print(f\"\\n{model_name} Results:\")\n",
    "        for metric, value in results.items():\n",
    "            print(f\"{metric}: {value}\")\n",
    "    \n",
    "    return model_results"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
